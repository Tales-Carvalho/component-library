{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param data_dir temporal data storage for local execution\n",
    "# @param file_name csv file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.environ.get('file_name', 'data.csv')\n",
    "data_dir= os.environ.get('data_dir', '../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forced-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.1.1\n",
      "  Downloading pyspark-3.1.1.tar.gz (212.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 212.3 MB 4.7 kB/s eta 0:00:01     |████████████████████████▊       | 163.8 MB 4.1 MB/s eta 0:00:12��████████▊    | 183.9 MB 4.4 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=7f666eab8a29b1d2f797395a094dc20483167c13d20c15295ff782ee95c84044\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/b3/0e/81/264aeed961e43b9f6ba9ec81c8c540d2d7dccc52c6b51cbf22\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark==3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innovative-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "superb-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "earned-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header','true').csv('../../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select min(time), max(time) from df').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bridal-dancing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----------+\n",
      "|      time|   lat|   lon|         sm|\n",
      "+----------+------+------+-----------+\n",
      "|1979-06-01|74.625|87.875| 0.20680334|\n",
      "|1979-07-01|74.625|87.875|  0.3784733|\n",
      "|1979-08-01|74.625|87.875| 0.43579885|\n",
      "|1979-09-01|74.625|87.875| 0.47746503|\n",
      "|1980-06-01|74.625|87.875|       0.04|\n",
      "|1980-07-01|74.625|87.875| 0.36691222|\n",
      "|1980-08-01|74.625|87.875| 0.39871296|\n",
      "|1980-09-01|74.625|87.875|   0.475062|\n",
      "|1981-06-01|74.625|87.875| 0.19585133|\n",
      "|1981-07-01|74.625|87.875| 0.33518302|\n",
      "|1981-08-01|74.625|87.875| 0.38786596|\n",
      "|1981-09-01|74.625|87.875| 0.41234347|\n",
      "|1982-06-01|74.625|87.875| 0.26879317|\n",
      "|1982-07-01|74.625|87.875| 0.37895218|\n",
      "|1982-08-01|74.625|87.875|  0.3972472|\n",
      "|1982-09-01|74.625|87.875| 0.45598736|\n",
      "|1983-06-01|74.625|87.875|0.077149324|\n",
      "|1983-07-01|74.625|87.875|  0.3582895|\n",
      "|1983-08-01|74.625|87.875| 0.41817418|\n",
      "|1983-09-01|74.625|87.875| 0.42353442|\n",
      "+----------+------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from df where sm <> 'null' and lat='74.625' and lon='87.875' order by time asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "practical-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1036800|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "select count(*) from (\n",
    "    select count(*) from df group by lon, lat\n",
    ")\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-alliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['87.875', '74.625']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select lon, lat from df where lat='74.625' and lon='87.875' group by lon, lat\n",
    "''').rdd.map(lambda x: [x.lon, x.lat]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "endless-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = spark.sql(\"select sm from df where sm <> 'null' and lat='74.625' and lon='87.875' order by time asc\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spatial-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sm = np.array(list(map(lambda x :x.sm, sm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "respected-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = sm.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "internal-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope 0.0004954139143477216\n",
      "NRMSE: 0.15286518423836323\n"
     ]
    }
   ],
   "source": [
    "coefficients, residuals, _, _, _ = np.polyfit(range(len(sm)),sm,1,full=True)\n",
    "mse = residuals[0]/(len(sm))\n",
    "nrmse = np.sqrt(mse)/(sm.max() - sm.min())\n",
    "print('Slope ' + str(coefficients[0]))\n",
    "print('NRMSE: ' + str(nrmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-twist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
